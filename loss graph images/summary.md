# CNN + Transformer

1. **초기 손실 경향**: CNN+Transformer 모델들은 초기 손실이 0.3577~0.5018 사이로 높은 값을 보였으며, 초기 학습 단계에서 다소 높은 손실로 시작한 후 빠르게 감소하였다. 이는 CNN과 Transformer의 조합이 학습 초기 단계에서부터 강력한 성능을 보이는 경향이 있음을 시사한다.
2. **손실 감소 추이**: CNN+Transformer 모델은 손실이 40 에포크 이후에 안정화되는 경향을 보였으며, 학습 후반부에는 미세한 손실 감소만 이루어졌다. 전반적으로 일정한 학습 속도로 수렴하며, 빠르게 손실을 줄이는 모습이 관찰된다.
3. **손실 변동성**: 일부 에포크에서 손실이 일시적으로 증가하는 경향이 있지만, 전반적으로 손실이 안정적으로 감소하는 추세를 유지했다. 이는 CNN과 Transformer의 조합이 강건한 학습을 지원하며, 변동성 또한 효과적으로 제어할 수 있음을 나타낸다.
4. **최종 손실 및 정확도**: CNN+Transformer 모델은 최종 손실이 약 0.0002~0.0003까지 감소했으며, 최종 정확도는 99.92%~100%로 매우 높은 성능을 기록했다. 이러한 결과는 CNN+Transformer 조합이 높은 정확도를 구현하는 데 탁월함을 보여준다.
5. **모델 저장 및 결과**: 두 모델 모두 학습을 성공적으로 마친 후 저장되었으며, 안정적으로 높은 성능을 발휘하는 결과를 보였다.

### 경향 분석

CNN+Transformer 모델들은 초기 학습 속도가 빠르고 최종적으로 높은 정확도에 도달하는 경향이 있다. 손실이 일정하게 감소하며, 최종 손실 값 또한 매우 낮아 안정적이고 강력한 성능을 보이는 모델임을 확인할 수 있다.

# CNN-GRU

1. **초기 손실 경향**: CNN-GRU 모델의 초기 손실은 0.5783~0.6956으로, 다소 높은 손실로 시작하였다. 초기 학습 단계에서는 손실 감소가 안정적이지 않은 모습을 보이며, 상대적으로 높은 변동성을 보였다.
2. **손실 감소 추이**: CNN-GRU 모델들은 전반적으로 꾸준한 손실 감소를 보였지만, 일부 모델은 40 에포크 이후에도 손실 변동이 지속되어 완전히 안정화되지 않았다. 후반부에서 손실이 서서히 안정화되지만, CNN-Transformer에 비해 손실 감소 추이가 비교적 불안정하다.
3. **손실 변동성**: CNN-GRU 모델들은 손실 감소 과정에서 약간의 상승과 하락을 반복하며 높은 변동성을 보였다. 특히 손실이 완만하게 감소하면서도 꾸준히 변동하는 양상이 두드러졌다.
4. **최종 손실 및 정확도**: CNN-GRU 모델은 최종 손실이 약 0.0387~0.5110 사이에서 마무리되었으며, 최종 정확도는 49.88%~77.60%로, 정확도 편차가 다소 컸다. 이는 CNN-GRU 구조가 데이터에 따라 더 많은 조정이 필요함을 나타낸다.
5. **모델 저장 및 결과**: 모델들은 각각 학습 후 저장되었으나, 성능 차이가 커 안정성에 있어서 다소 개선의 여지가 있음을 시사한다.

### 경향 분석

CNN-GRU 모델들은 손실 감소 속도가 일정하지 않으며, 정확도 편차가 크게 나타난다. 일부 모델은 높은 정확도와 낮은 손실로 안정적 학습을 보였지만, 다른 모델은 비교적 불안정한 학습 패턴을 보였다. CNN-GRU는 초기 학습 속도가 느리고 학습 안정성을 위해 추가적인 튜닝이 필요한 경향이 있다.

# CNN-LSTM

1. **초기 손실 경향**: CNN-LSTM 모델의 초기 손실은 0.5947~0.6982로 다소 높은 수준으로 시작되었다. 초기 학습 단계에서 손실 감소가 원활히 이루어지지 않는 경향을 보이며, 초기 학습 단계에서 수렴 속도가 느렸다.
2. **손실 감소 추이**: CNN-LSTM 모델들은 전반적으로 손실이 완만히 감소하였으나, 중간에 일부 손실이 증가하는 구간이 있어 손실이 일관되게 줄어들지는 않았다. 학습 후반부에서는 손실이 다소 감소하며 안정화되었으나, 일부 모델에서는 마지막 에포크까지 손실이 일정하게 낮아지지 않았다.
3. **손실 변동성**: CNN-LSTM 모델들은 학습 도중 손실이 증가하거나 감소하는 불안정한 변동성을 보였으며, 에포크에 따라 손실 변동 폭이 크게 나타났다. 이는 CNN-LSTM 조합이 CNN-Transformer보다 상대적으로 학습 안정성이 낮음을 시사한다.
4. **최종 손실 및 정확도**: CNN-LSTM 모델의 최종 손실은 0.2717~0.5752로 마무리되었으며, 최종 정확도는 65.06%~87.02%로 높은 편이지만, 모델 간 성능 차이가 크게 나타났다. 정확도 편차가 크다는 점에서 CNN-LSTM 모델의 학습이 데이터나 초기화에 영향을 많이 받는 것으로 보인다.
5. **모델 저장 및 결과**: 학습을 마친 후 모델들은 모두 저장되었으며, 정확도와 손실에서 다소 편차가 크더라도 각각 학습 완료 상태로 안정된 성능을 기록했다.

### 경향 분석

CNN-LSTM 모델들은 손실 감소에서 불안정한 변동성을 보이며, 학습 과정이 CNN-Transformer보다 다소 불안정하고 수렴 속도가 느리다. 정확도 편차가 크다는 점에서 모델 성능이 데이터 특성이나 하이퍼파라미터 설정에 민감한 경향을 나타내며, 학습 안정성을 높이기 위한 추가적인 조정이 필요할 수 있다.

# 종합 결론

각 모델들의 구조와 성능 경향을 종합적으로 분석한 결과, CNN-Transformer 모델들이 초기 수렴 속도가 빠르고 학습 안정성이 뛰어나 높은 성능을 기록하는 데 가장 유리한 경향이 있다. CNN-GRU와 CNN-LSTM은 특정 조건에서 높은 성능을 발휘하기도 하지만, 학습 과정에서 불안정성이 다소 커 모델 조정이 요구될 수 있다.

# 모델별 수렴 값

| 모델               | 수렴 손실 값 | 수렴 에포크 |
|--------------------|--------------|-------------|
| CNN + Transformer  | 0.0002       | 40          |
| CNN-GRU            | 0.0387       | 80          |
| CNN-LSTM           | 0.2717       | 90          |

# 이전과 현재 정확도 비교

| 모델               | 이전 정확도  | 현재 정확도 |
|--------------------|-------------|-------------|
| CNN + Transformer  | 97.56%      | 99.92%      |
| CNN-GRU            | 58.54%      | 77.60%      |
| CNN-LSTM           | 60.86%      | 87.02%      |

# 이전과 현재 수렴 에포크 경향
| 모델               | 이전 수렴 에포크 | 현재 수렴 에포크 |
|--------------------|------------------|------------------|
| CNN + Transformer  | 10+              | 40              |
| CNN-GRU            | 10+              | 80              |
| CNN-LSTM           | 10+              | 90              |

**이전 모델은 loss 값이 수렴하기 전에 학습을 종료함**